{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating the Gürkaynak, Sack, and Wright (2006) Treasury Yield Curve\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section, we'll explore how to replicate the U.S. Treasury yield curve estimation methodology developed by Gürkaynak, Sack, and Wright (2006) (hereafter GSW). The GSW yield curve has become a standard benchmark in both academic research and industry practice. Their approach provides daily estimates of the U.S. Treasury yield curve from 1961 to the present, making it an invaluable resource for analyzing historical interest rate dynamics.\n",
    "\n",
    "## The Nelson-Siegel-Svensson Model\n",
    "\n",
    "The GSW methodology employs the Nelson-Siegel-Svensson (NSS) model to fit the yield curve. The NSS model expresses instantaneous forward rates using a flexible functional form with six parameters:\n",
    "\n",
    "Example: NSS Forward Rate Function\n",
    "The instantaneous forward rate n years ahead is given by:\n",
    "\n",
    "$$\n",
    "f(n) = \\beta_1 + \\beta_2 e^{-n/\\tau_1} + \\beta_3\\left(\\frac{n}{\\tau_1}\\right)e^{-n/\\tau_1} + \\beta_4\\left(\\frac{n}{\\tau_2}\\right)e^{-n/\\tau_2}\n",
    "$$\n",
    "\n",
    "\n",
    "This specification allows for rich curve shapes while maintaining smoothness and asymptotic behavior. The parameters have intuitive interpretations:\n",
    "- $\\beta_1$: The asymptotic forward rate\n",
    "- $\\beta_2$, $\\beta_3$, $\\beta_4$: Control the shape and humps of the curve\n",
    "- $\\tau_1$, $\\tau_2$: Determine the location of curve features\n",
    "\n",
    "$$\n",
    "y(t) = \\beta_1 + \\beta_2\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}\\right) + \\beta_3\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1} - e^{-t/\\tau_1}\\right) + \\beta_4\\left(\\frac{1-e^{-t/\\tau_2}}{t/\\tau_2} - e^{-t/\\tau_2}\\right)\n",
    "$$\n",
    "\n",
    "This equation shows the zero-coupon yield $y(t)$ for maturity $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pull_CRSP_treasury\n",
    "import pull_yield_curve_data\n",
    "import gsw2006_yield_curve\n",
    "from settings import config\n",
    "\n",
    "DATA_DIR = config(\"DATA_DIR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nelson-Siegel-Svensson parameters\n",
    "# \"tau1\", \"tau2\", \"beta1\", \"beta2\", \"beta3\", \"beta4\"\n",
    "params = np.array([1.0, 10.0, 3.0, 3.0, 3.0, 3.0])\n",
    "\n",
    "gsw2006_yield_curve.plot_spot_curve(params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nelson-Siegel-Svensson parameters\n",
    "# \"tau1\", \"tau2\", \"beta1\", \"beta2\", \"beta3\", \"beta4\"\n",
    "params = np.array([1.0, 10.0, 3.0, 3.0, 3.0, 30.0])\n",
    "\n",
    "gsw2006_yield_curve.plot_spot_curve(params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Foundations\n",
    "\n",
    "The Nelson-Siegel-Svensson model is commonly used in practice to fit the yield curve. It has statistically appealing properties, but it is not arbitrage-free. Here's a detailed breakdown of why:\n",
    "\n",
    "\n",
    "### 1. **Static Curve-Fitting Approach**\n",
    "   - The NSS model is primarily a **parametric curve-fitting tool** that focuses on matching observed yields at a single point in time. \n",
    "   - It does not model the **dynamic evolution of interest rates** or enforce consistency between short-term rate expectations and long-term yields over time, a key requirement for no-arbitrage models.\n",
    "\n",
    "### 2. **Absence of No-Arbitrage Restrictions**\n",
    "   - No-arbitrage models impose constraints to prevent risk-free profits. For example, affine term structure models derive bond prices from:\n",
    "     \n",
    "     $$\n",
    "     P(t,T) = \\mathbb{E}^\\mathbb{Q}\\left[e^{-\\int_t^T r_s ds}\\right],\n",
    "     $$\n",
    "\n",
    "     where $\\mathbb{Q}$ is the risk-neutral measure. The NSS model lacks such theoretical foundations.\n",
    "   - The NSS parameters (e.g., level, slope, curvature) are **statistically estimated** rather than derived from economic principles or arbitrage-free dynamics.\n",
    "\n",
    "### 3. **Factor Dynamics and Risk Premiums**\n",
    "   - In arbitrage-free models, factor dynamics (e.g., mean reversion) and risk premiums are explicitly defined to ensure consistency across maturities. The NSS model treats factors as **latent variables** without specifying their stochastic behavior or market price of risk.\n",
    "   - This omission allows potential inconsistencies between short-rate expectations and long-term yields, creating theoretical arbitrage opportunities.\n",
    "\n",
    "### 4. **Contrast with Arbitrage-Free Extensions**\n",
    "   - The **arbitrage-free Nelson-Siegel (AFNS)** model, developed by Christensen et al. (2007), addresses these limitations by:\n",
    "     - Embedding Nelson-Siegel factors into a dynamic arbitrage-free framework.\n",
    "     - Explicitly defining factor dynamics under both physical ($\\mathbb{P}$) and risk-neutral ($\\mathbb{Q}$) measures.\n",
    "     - Ensuring internal consistency between yields of different maturities.\n",
    "\n",
    "### 5. **Empirical vs. Theoretical Focus**\n",
    "   - The NSS model prioritizes **empirical flexibility** (e.g., fitting yield curve shapes like humps) over theoretical rigor. While it performs well in practice, this trade-off inherently sacrifices no-arbitrage guarantees.\n",
    "\n",
    "In summary, the NSS model’s lack of dynamic factor specifications, absence of explicit no-arbitrage constraints, and focus on cross-sectional fitting rather than intertemporal consistency render it theoretically incompatible with arbitrage-free principles. Its successors, such as the AFNS model, bridge this gap by integrating no-arbitrage restrictions while retaining empirical tractability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering\n",
    "\n",
    "One important step of the GSW methodology is careful filtering of Treasury securities. \n",
    "\n",
    "The following filters are implemented:\n",
    "\n",
    "1. Exclude securities with < 3 months to maturity\n",
    "2. Exclude on-the-run and first off-the-run issues after 1980\n",
    "3. Exclude T-bills (only keep notes and bonds)\n",
    "4. Exclude 20-year bonds after 1996 with decay\n",
    "5. Exclude callable bonds\n",
    "\n",
    "The GSW paper also includes ad hoc exclusions for specific issues, which are not implemented here.\n",
    "\n",
    "Why are these filters important?\n",
    "\n",
    "For (2), this is what the paper says:\n",
    "\n",
    "> We exclude the two most recently issued securities with maturities of two,\n",
    "> three, four, five, seven, ten, twenty, and thirty years for securities\n",
    "> issued in 1980 or later. These are the \"on-the-run\" and \"first off-the-run\"\n",
    "> issues that often trade at a premium to other Treasury securities, owing to\n",
    "> their greater liquidity and their frequent specialness in the repo market.8\n",
    "> Earlier in the sample, the concept of an on-the-run issue was not well\n",
    "> defined, since the Treasury did not conduct regular auctions and the repo\n",
    "> market was not well developed (as discussed by Garbade (2004)). Our cut-off\n",
    "> point for excluding onthe- run and first off-the-run issues is somewhat\n",
    "> arbitrary but is a conservative choice (in the sense of potentially erring\n",
    "> on the side of being too early).\n",
    "\n",
    "For (4), this is what the paper says:\n",
    "\n",
    "> We begin to exclude twenty-year bonds in 1996, because those securities\n",
    "> often appeared cheap relative to ten-year notes with comparable duration.\n",
    "> This cheapness could reflect their lower liquidity or the fact that their\n",
    "> high coupon rates made them unattractive to hold for tax-related reasons.\n",
    ">\n",
    "> To avoid an abrupt change to the sample, we allow their weights to linearly\n",
    "> decay from 1 to 0 over the year ending on January 2, 1996.\n",
    "\n",
    "Let's examine how we implement these filters using CRSP data. The following is\n",
    "from the `pull_CRSP_treasury.py` file:\n",
    "\n",
    "```python\n",
    "def gurkaynak_sack_wright_filters(dff):\n",
    "    \"\"\"Apply Treasury security filters based on Gürkaynak, Sack, and Wright (2006).\n",
    "    \"\"\"\n",
    "    df = dff.copy()\n",
    "\n",
    "    # Filter 1: Exclude < 3 months to maturity\n",
    "    df = df[df[\"days_to_maturity\"] > 92]\n",
    "\n",
    "    # Filter 2: Exclude on-the-run and first off-the-run after 1980\n",
    "    post_1980 = df[\"caldt\"] >= pd.to_datetime(\"1980-01-01\")\n",
    "    df = df[~(post_1980 & (df[\"run\"] <= 2))]\n",
    "\n",
    "    # Filter 3: Only include notes (2) and bonds (1)\n",
    "    df = df[df[\"itype\"].isin([1, 2])]\n",
    "\n",
    "    # Filter 4: Exclude 20-year bonds after 1996 with decay\n",
    "    cutoff_date = pd.to_datetime(\"1996-01-02\")\n",
    "    decay_start = cutoff_date - pd.DateOffset(years=1)\n",
    "\n",
    "    df[\"weight\"] = 1.0\n",
    "    # Apply linear decay only during 1995-01-02 to 1996-01-02\n",
    "    mask_decay = (\n",
    "        (df[\"original_maturity\"] == 20)\n",
    "        & (df[\"caldt\"] >= decay_start)\n",
    "        & (df[\"caldt\"] <= cutoff_date)\n",
    "    )\n",
    "    # Calculate proper decay factor for the transition year\n",
    "    decay_days = (cutoff_date - decay_start).days\n",
    "    decay_factor = 1 - ((df[\"caldt\"] - decay_start).dt.days / decay_days)\n",
    "    df.loc[mask_decay, \"weight\"] *= decay_factor\n",
    "\n",
    "    # Completely exclude 20-year bonds after cutoff date\n",
    "    mask_exclude = (df[\"original_maturity\"] == 20) & (df[\"caldt\"] > cutoff_date)\n",
    "    df.loc[mask_exclude, \"weight\"] = 0\n",
    "\n",
    "    # Filter 5: Exclude callable bonds\n",
    "    df = df[~df[\"callable\"]]\n",
    "\n",
    "    # Remove securities with zero/negative weights\n",
    "    df = df[df[\"weight\"] > 0]\n",
    "\n",
    "    return df\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how this affects the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Gurkaynak Sack Wright data from Federal Reserve's website\n",
    "# See here: https://www.federalreserve.gov/data/nominal-yield-curve.htm\n",
    "# and here: https://www.federalreserve.gov/data/yield-curve-tables/feds200628_1.html\n",
    "actual_all = pull_yield_curve_data.load_fed_yield_curve_all(data_dir=DATA_DIR)\n",
    "# Create copy of parameter DataFrame to avoid view vs copy issues\n",
    "actual_params_all = actual_all.loc[\n",
    "    :, [\"TAU1\", \"TAU2\", \"BETA0\", \"BETA1\", \"BETA2\", \"BETA3\"]\n",
    "].copy()\n",
    "# Convert percentage points to decimals for beta parameters\n",
    "beta_columns = [\"BETA0\", \"BETA1\", \"BETA2\", \"BETA3\"]\n",
    "actual_params_all[beta_columns] = actual_params_all[beta_columns] / 100\n",
    "\n",
    "\n",
    "## Load CRSP Treasury data from Wharton Research Data Services\n",
    "# We will fit a Nelson-Siegel-Svensson model to this data to see\n",
    "# if we can replicate the Gurkaynak Sack Wright results above.\n",
    "df_all = pull_CRSP_treasury.load_CRSP_treasury_consolidated(data_dir=DATA_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = gsw2006_yield_curve.gurkaynak_sack_wright_filters(df_all)\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Steps\n",
    "\n",
    "### 1. Data Preparation\n",
    "First, we load and clean the CRSP Treasury data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pull_CRSP_treasury.load_CRSP_treasury_consolidated(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cashflow Construction\n",
    "For each Treasury security, we need to calculate its future cashflows. Consider the following simplified example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"tcusip\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "        \"tmatdt\": pd.to_datetime(\n",
    "            [\"2000-05-15\", \"2000-05-31\", \"2000-06-30\", \"2000-07-31\", \"2000-08-15\"]\n",
    "        ),\n",
    "        \"price\": [101, 101, 100, 100, 103],\n",
    "        \"tcouprt\": [6, 6, 0, 5, 6],\n",
    "        \"caldt\": pd.to_datetime(\"2000-01-31\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "cashflow = gsw2006_yield_curve.calc_cashflows(sample_data)\n",
    "\n",
    "# Treasury securities have 2 coupon payments per year\n",
    "# and pay their final coupon and principal on the maturity date\n",
    "expected_cashflow = np.array(\n",
    "    [\n",
    "        [0.0, 103.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 103.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 100.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 102.5, 0.0],\n",
    "        [3.0, 0.0, 0.0, 0.0, 0.0, 103.0],\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cashflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Fitting\n",
    "The NSS model is fit by minimizing price errors weighted by duration:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta,\\tau} \\sum_{i=1}^N \\frac{(P_i^{obs} - P_i^{model})^2}{D_i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P_i^{obs}$ = Observed clean price (including accrued interest)\n",
    "- $P_i^{model}$ = Model-implied price\n",
    "- $D_i$ = Duration of security i\n",
    "\n",
    "Now, why are the squared errors weighted by the duration?\n",
    "\n",
    "Recall that bond duration is a measurement of how much a bond's price will change in response to interest rate changes. \n",
    "Thus, the price error objective is approximately equivalent to minimizing unweighted yield errors:\n",
    "\n",
    "$$\n",
    "\\frac{(P_i^{obs} - P_i^{model})^2}{D_i} \\approx D_i \\cdot (y_i^{obs} - y_i^{model})^2\n",
    "$$\n",
    "\n",
    "This approximation comes from the duration relationship:\n",
    "$$\n",
    "P^{obs} - P^{model} \\approx -D \\cdot (y^{obs} - y^{model})\n",
    "$$\n",
    "\n",
    "Making the objective function:\n",
    "$$\n",
    "\\sum D_i \\cdot (y_i^{obs} - y_i^{model})^2\n",
    "$$\n",
    "\n",
    "So, why Price Errors Instead of Yield Errors?\n",
    "\n",
    "1. **Non-linear relationship**: The price/yield relationship is convex\n",
    "    (convexity adjustment matters more for long-duration bonds)\n",
    "2. **Coupon effects**: Directly accounts for differential cash flow timing\n",
    "3. **Numerical stability**: Prices have linear sensitivity to parameters via\n",
    "    discount factors, while yields require non-linear root-finding\n",
    "4. **Economic meaning**: Aligns with trader behavior that thinks in terms of\n",
    "    price arbitrage\n",
    "\n",
    "Reference: Gurkaynak, Sack, and Wright (2006)\n",
    "\n",
    "### Note\n",
    "\n",
    "Note that the paper says the following:\n",
    "\n",
    "> In estimating the yield curve, we choose the parameters to minimize the weighted\n",
    "> sum of the squared deviations between the actual prices of Treasury securities and the\n",
    "> predicted prices. The weights chosen are the inverse of the duration of each individual\n",
    "> security. To a rough approximation, the deviation between the actual and predicted\n",
    "> prices of an individual security will equal its duration multiplied by the deviation\n",
    "> between the actual and predicted yields. Thus, this procedure is approximately equal to\n",
    "> minimizing the (unweighted) sum of the squared deviations between the actual and\n",
    "> predicted yields on all of the securities.\n",
    "\n",
    "However, I need to check this further since it initially seems to me that this procedure is minimizing the squared yield errors, weighted by the duration. However, the fit to actual prices seems better with the procedure above. I need to check this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Testing and Validation\n",
    "\n",
    "To validate our implementation, we compare our fitted yields against the official GSW yields published by the Federal Reserve:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Load Gurkaynak Sack Wright data from Federal Reserve's website\n",
    "# See here: https://www.federalreserve.gov/data/nominal-yield-curve.htm\n",
    "# and here: https://www.federalreserve.gov/data/yield-curve-tables/feds200628_1.html\n",
    "actual_all = pull_yield_curve_data.load_fed_yield_curve_all(data_dir=DATA_DIR)\n",
    "# Create copy of parameter DataFrame to avoid view vs copy issues\n",
    "actual_params_all = actual_all.loc[\n",
    "    :, [\"TAU1\", \"TAU2\", \"BETA0\", \"BETA1\", \"BETA2\", \"BETA3\"]\n",
    "].copy()\n",
    "# Convert percentage points to decimals for beta parameters\n",
    "beta_columns = [\"BETA0\", \"BETA1\", \"BETA2\", \"BETA3\"]\n",
    "actual_params_all[beta_columns] = actual_params_all[beta_columns] / 100\n",
    "\n",
    "\n",
    "## Load CRSP Treasury data from Wharton Research Data Services\n",
    "# We will fit a Nelson-Siegel-Svensson model to this data to see\n",
    "# if we can replicate the Gurkaynak Sack Wright results above.\n",
    "df_all = pull_CRSP_treasury.load_CRSP_treasury_consolidated(data_dir=DATA_DIR)\n",
    "df_all = gsw2006_yield_curve.gurkaynak_sack_wright_filters(df_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_dates = pd.date_range(\"2000-01-02\", \"2024-06-30\", freq=\"BMS\")\n",
    "# quote_date = quote_dates[-1]\n",
    "\n",
    "## Test Day 1\n",
    "quote_date = pd.to_datetime(\"2024-06-03\")\n",
    "# Subset df_all to quote_date\n",
    "df = df_all[df_all[\"caldt\"] == quote_date]\n",
    "actual_params = actual_params_all[actual_params_all.index == quote_date].values[0]\n",
    "\n",
    "# \"tau1\", \"tau2\", \"beta1\", \"beta2\", \"beta3\", \"beta4\"\n",
    "# params0 = np.array([1.0, 10.0, 3.0, 3.0, 3.0, 3.0])\n",
    "params0 = np.array([0.989721, 9.955324, 3.685087, 1.579927, 3.637107, 9.814584])\n",
    "# params0 = np.array([1.0, 1.0, 0.001, 0.001, 0.001, 0.001])\n",
    "\n",
    "params_star, error = gsw2006_yield_curve.fit(quote_date, df_all, params0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the fit\n",
    "gsw2006_yield_curve.plot_spot_curve(params_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsw2006_yield_curve.plot_spot_curve(actual_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_comparison = gsw2006_yield_curve.compare_fit(\n",
    "    quote_date, df_all, params_star, actual_params, df\n",
    ")\n",
    "price_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assert that column is close to 0 for all CUSIPs\n",
    "assert (price_comparison[\"Predicted - Actual %\"].abs() < 0.05).all()\n",
    "assert (price_comparison[\"Predicted - GSW %\"].abs() < 0.02).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Test Day 2\n",
    "quote_date = pd.to_datetime(\"2000-06-05\")\n",
    "# Subset df_all to quote_date\n",
    "df = df_all[df_all[\"caldt\"] == quote_date]\n",
    "actual_params = actual_params_all[actual_params_all.index == quote_date].values[0]\n",
    "\n",
    "# \"tau1\", \"tau2\", \"beta1\", \"beta2\", \"beta3\", \"beta4\"\n",
    "# params0 = np.array([1.0, 10.0, 3.0, 3.0, 3.0, 3.0])\n",
    "params0 = np.array([0.989721, 9.955324, 3.685087, 1.579927, 3.637107, 9.814584])\n",
    "# params0 = np.array([1.0, 1.0, 0.001, 0.001, 0.001, 0.001])\n",
    "\n",
    "params_star, error = gsw2006_yield_curve.fit(quote_date, df_all, params0)\n",
    "\n",
    "## Visualize the fit\n",
    "# gsw2006_yield_curve.plot_spot_curve(params_star)\n",
    "# gsw2006_yield_curve.plot_spot_curve(actual_params)\n",
    "\n",
    "price_comparison = gsw2006_yield_curve.compare_fit(\n",
    "    quote_date, df_all, params_star, actual_params, df\n",
    ")\n",
    "\n",
    "## Assert that column is close to 0 for all CUSIPs\n",
    "assert (price_comparison[\"Predicted - Actual %\"].abs() < 0.05).all()\n",
    "assert (price_comparison[\"Predicted - GSW %\"].abs() < 0.02).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Day 3\n",
    "quote_date = pd.to_datetime(\"1990-06-05\")\n",
    "# Subset df_all to quote_date\n",
    "df = df_all[df_all[\"caldt\"] == quote_date]\n",
    "actual_params = actual_params_all[actual_params_all.index == quote_date].values[0]\n",
    "\n",
    "# \"tau1\", \"tau2\", \"beta1\", \"beta2\", \"beta3\", \"beta4\"\n",
    "# params0 = np.array([1.0, 10.0, 3.0, 3.0, 3.0, 3.0])\n",
    "params0 = np.array([0.989721, 9.955324, 3.685087, 1.579927, 3.637107, 9.814584])\n",
    "# params0 = np.array([1.0, 1.0, 0.001, 0.001, 0.001, 0.001])\n",
    "\n",
    "params_star, error = gsw2006_yield_curve.fit(quote_date, df_all, params0)\n",
    "\n",
    "## Visualize the fit\n",
    "# gsw2006_yield_curve.plot_spot_curve(params_star)\n",
    "# gsw2006_yield_curve.plot_spot_curve(actual_params)\n",
    "\n",
    "price_comparison = gsw2006_yield_curve.compare_fit(\n",
    "    quote_date, df_all, params_star, actual_params, df\n",
    ")\n",
    "\n",
    "## Assert that column is close to 0 for all CUSIPs\n",
    "assert (price_comparison[\"Predicted - Actual %\"].abs() < 0.05).all()\n",
    "assert (price_comparison[\"Predicted - GSW %\"].abs() < 0.02).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The GSW yield curve methodology provides a robust framework for estimating the U.S. Treasury yield curve. By carefully implementing their filtering criteria and optimization approach, we can replicate their results with high accuracy. This implementation allows us to extend their analysis to current data and provides a foundation for various fixed-income applications.\n",
    "\n",
    "Example: Model Performance\n",
    "Our implementation typically achieves price errors below 0.02% compared to the official GSW yields, demonstrating the reliability of the replication."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
